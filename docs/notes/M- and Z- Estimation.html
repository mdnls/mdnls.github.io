
<!DOCTYPE html>
    <head>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="../styles/styles.css">
        <link rel="icon" href="assets/icon.ico" />
        <meta name="google-site-verification" content="gGsyj98Hmr9mZj0-DQzJIAdOP3eJXZJZnM4_NF8Mai8" />
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"> 
        <title>Mara Daniels - MIT</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                ],
                macros: {
                    '\\saeq': '\\{#1\\}',
                    '\\linf': '\\lim_{n \\to \\infty}',
                    '\\Alpha': '\\mathcal{A}',
                    '\\norm': '\\left\\lVert#1\\right\\rVert',
                    '\\ct': 'C^\\infty_c ( #1 )',
                    '\\lloc': 'L^1_{\\mathsf{loc}}( #1 )',
                    '\\Scr': '\\mathscr{S}',
                    '\\tmpr': '\\mathscr{S}',
                    '\\limnf': '\\liminf \\limits_{#1}',
                    '\\limsp': '\\limsup \\limits_{#1}',
                    '\\R': '{\\mathbb R}',
                    '\\C': '{\\mathbb C}',
                    '\\N': '{\\mathbb N}',
                    '\\Q': '{\\mathbb Q}',
                    '\\H': '{\\mathbb H}',
                    '\\S': '{\\mathbb S}',
                    '\\Z': '{\\mathbb Z}',
                    '\\E': '{\\mathbb E}',
                    '\\Var': '\\mathsf{Var}',
                    '\\Cov': '\\mathsf{Cov}',
                    '\\KL': '\\mathsf{KL}',
                    '\\Ent': '\\mathsf{Ent}',
                    '\\F': '{\\mathbb F}',
                    '\\Acl': '\\mathcal{A}',
                    '\\Bcl': '\\mathcal{B}',
                    '\\Ccl': '\\mathcal{C}',
                    '\\Dcl': '\\mathcal{D}',
                    '\\Ecl': '\\mathcal{E}',
                    '\\Fcl': '\\mathcal{F}',
                    '\\Gcl': '\\mathcal{G}',
                    '\\Hcl': '\\mathcal{H}',
                    '\\Jcl': '\\mathcal{J}',
                    '\\Kcl': '\\mathcal{K}',
                    '\\Lcl': '\\mathcal{L}',
                    '\\Mcl': '\\mathcal{M}',
                    '\\Ncl': '\\mathcal{N}',
                    '\\Ocl': '\\mathcal{O}',
                    '\\Pcl': '\\mathcal{P}',
                    '\\Qcl': '\\mathcal{Q}',
                    '\\Rcl': '\\mathcal{R}',
                    '\\Scl': '\\mathcal{S}',
                    '\\Tcl': '\\mathcal{T}',
                    '\\Ucl': '\\mathcal{U}',
                    '\\Vcl': '\\mathcal{V}',
                    '\\Wcl': '\\mathcal{W}',
                    '\\Xcl': '\\mathcal{X}',
                    '\\Ycl': '\\mathcal{Y}',
                    '\\Zcl': '\\mathcal{Z}',
                    '\\diag': '\\mathrm{diag}',
                    '\\supp': '\\mathrm{supp}',
                    '\\diam': '\\mathrm{diam}',
                    '\\sgn': '\\mathrm{sgn}',
                    '\\im': '\\mathrm{Im}',
                    '\\lcm': '\\mathrm{lcm}',
                    '\\aut': '\\mathrm{Aut}',
                    '\\inn': '\\mathrm{Inn}',
                    '\\rg': '\\mathrm{rg}',
                    '\\vol': '\\mathrm{vol}',
                    '\\Pr': '\\mathrm{Pr}',
                    '\\Tr': '\\mathrm{Tr}',
                    '\\eps': '\\varepsilon',
                    '\\charfct': '\\mathds{1}',
                    '\\nullfct': '{\\bf 0}',
                    '\\as': '\\text{a.s.}\\xspace',
                    '\\argmax': '\\mathop{\\mathrm{arg\\,max}}',
                    '\\argmin': '\\mathop{\\mathrm{arg\\,min}}',
                    '\\esssup': '\\mathop{\\mathrm{ess\\,sup}}',
                    '\\th': '\\hat{\\theta}',
                    '\\pto': '\\stackrel{p}{\\longrightarrow}',
                    '\\dto': '\\stackrel{d}{\\longrightarrow}',
                    '\\asto': '\\stackrel{a.s.}{\\longrightarrow}',
                    '\\coloneqq': ':='
                }
            });">
            </script>
    </head>
    <body>
        <div class="row">
            <div class="col-lg-12 col-xl-8 offset-xl-2"> 
            <h1>M- and Z- Estimation</h1>
<p>
In a statistical estimation problem, such as <a href="Maximum Likelihood.html">maximum likelihood estimation</a>, it is often desirable to establish asymptotic properties of an estimator like consistency or asymptotic normality. M-estimators and Z-estimators are two broad classes of estimators for which these two results can be proven easily.
</p>
<hr />
<p>
<strong>Definition (m-estimation, z-estimation)</strong>. let $X_1, \ldots, X_n \sim p_{\theta*}$ i.i.d. where $\theta^* \in \Theta$ is a parameter belonging to the set $\Theta \subseteq \R^d$.
</p>
<ol>
<li>An M-estimator is defined in terms of a loss function $l_\theta(x) : \Theta \times \R^d \to \R$ and has the form, $$ \hat{\theta}_M = \arg\min_{\theta \in \Theta} \underbrace{\frac{1}{n}\sum_{i=1}^n l_\theta(X_i)}_{\hat{L}_n(\theta)}$$We call $L(\theta) = \E_{p_{\theta^*}}[{l_\theta(X)}]$ the <em>population loss</em> and $\hat{L}_n(\theta)$ the <em>empirical loss</em>.
</li>
<li>A Z-estimator is defined in terms of a function $F_\theta(x) : \Theta \times \R^d \to \R^d$ and has the form, $$ \hat{\theta}_Z \in \left\{\theta \in \Theta : \frac{1}{n}\sum_{i=1}^n F_\theta(X_i) = 0\right\}$$
</li>
</ol>
<p>
To each M-estimator, one can associate a canonical Z-estimator by taking $F_\theta(x) = \nabla_\theta l_\theta(x)$.
</p>
<hr />
<p>
If the empirical loss has multiple local minima, then the Z-estimator solution might not uniquely identify $\hat{\theta}$, so an additional selection rule is necessary to make the estimator well-specified. In this situation, it is also not guaranteed that the M-estimator solution is unique, and it is not guaranteed that the M- and Z- estimators agree.
</p>
<h2 id="uniform-convergence-consistency-of-m-estimation">Uniform Convergence & Consistency of M-Estimation</h2>
<p>
In the following proposition, we give a general result on the consistency of M-estimators whenever $l_\theta(x)$ is <em>uniformly convergent</em>. To prove consistency of Z-estimators, which is a considerably more general class, one often needs a problem-dependent approach. However, as we will show in the next section, it is easy to prove asymptotic normality of a consistent Z-estimator, which also applies to M-estimation when $\theta \mapsto \hat{L}_n(\theta)$ is convex.
</p>
<hr />
<p>
<strong>Proposition (consistency of m-estimation under uniform convergence)</strong>. Consider an M-estimation problem for which $\theta^* = \arg \min_{\theta \in \Theta}L(\theta)$ and suppose that $l_\theta(x)$ satisfies the <em>uniform convergence</em> property:
$$
\sup_{\theta \in \Theta} \left\{ \hat{L}_n(\theta) - L(\theta)\right\} \stackrel{p}{\to} 0 \qquad \text{ as } n \to \infty
$$
Then, $L(\hat{\theta}_M) \stackrel{p}{\to} L(\theta^*)$ as $n\to \infty$.
</p>
<hr />
<p>
<em>Proof</em>. write
$$
L(\hat{\theta}_M) - L(\theta^*) = \underbrace{ L(\hat{\theta}_M) - \hat{L}_n(\hat{\theta}_M)}_{(1)} + \underbrace{\hat{L}_n(\hat{\theta}_M) - \hat{L}_n(\theta^*)}_{\leq 0} + \underbrace{\hat{L}_n(\theta^*) - L(\theta^*)}_{(2)}
$$
and observe that $(1), (2) \stackrel{p}{\to} 0$ as $n \to \infty$ by uniform convergence. <span class="qed" />
</p>
<hr />
<p>
<strong>Proposition (uniform convergence conditions for m-estimation)</strong>. Suppose that $l_\theta(x)$ satisfies the condition, $$ |l_\theta(x)-l_{\theta'}(x)| \leq R(x)\phi(\|\theta - \theta'\|)$$where:
</p>
<ol>
<li>$R(x)$ is integrable with respect to the data generating distribution
</li>
<li>$\varphi(0)=0$ and $\varphi(\cdot)$ is continuous at $0$.
</li>
<li>$\| \cdot \|$ is a norm under which $\Theta$ is compact.
</li>
</ol>
<p>
Then $l_\theta(x)$ satisfies uniform convergence.
</p>
<hr />
<p>
<em>Proof</em>. the general strategy is to round $\Theta$ to a finite set, since it is easy to prove uniform convergence for discrete sets. To that end, for any $\eps > 0$, let $\delta > 0$ such that $|t|<\delta \implies \varphi(t)< \eps^2$. Let $\bar{\Theta}$ be a $\delta$-covering of $\Theta$ in $\|\cdot\|$ and let $\pi(\theta) : \Theta \to \bar{\Theta}$ be the projection map onto $\bar{\Theta}$.
</p>
<p>
For any $\theta \in \Theta$,
$$
\hat{L}_n(\theta) - L(\theta) = \underbrace{\hat{L}_n(\theta) - \hat{L}_n(\pi(\theta))}_{(1)} + \underbrace{\hat{L}_n(\pi(\theta)) - L(\pi(\theta))}_{(2)} + \underbrace{L(\pi(\theta)) - L(\theta)}_{(3)}
$$
</p>
<p>
The term $(1)$ satisfies
$$
|\hat{L}_n(\theta) - \hat{L}_n(\pi(\theta))| \leq \frac{1}{n}\sum_{i=1}^n |l_{\theta}(X_i) - l_{\pi(\theta)}(X_i) | \leq \left( \frac{1}{n} \sum_{i=1}^n R(X_i) \right) \phi(\|\theta - \pi(\theta)\|) \leq \eps^2 \left( \frac{1}{n} \sum_{i=1}^n R(X_i) \right)
$$
and so by Markov's inequality,
$$
P(\sup_{\theta \in \Theta} |\hat{L}_n(\theta) - \hat{L}_n(\pi(\theta))| < \eps) \geq 1 - P\left(\eps^2 \frac{1}{n}\sum_{i=1}^n R(X_i) \geq \eps \right) \geq 1 - \eps \E[R(X)].
$$
The same argument applies to $(3)$. For $(2)$, observe that
$$
P(\sup_{\theta \in \Theta}|\hat{L}_n(\pi(\theta)) - L(\pi(\theta))| > \eps) \leq |\bar{\Theta}| \max_{\theta \in \bar{\Theta}} P(|\hat{L}_n(\theta) - L(\theta)| > \eps) \stackrel{(n \gg 1)}{\leq }\eps
$$
where, in the last inequality, we use the law of large numbers to conclude that it holds for $n$ sufficiently large. Putting all of this together, it holds with probability at least $1 - (2 + \E[R])\eps$ that
$$
\sup_{\theta \in \Theta}|\hat{L}_n(\theta) - L(\theta)| \leq 3 \eps
$$
when $n$ is sufficiently large, so the claim follows. <span class="qed" />
</p>
<h2 id="asymptotic-normality-of-z--and-m--estimation">Asymptotic Normality of Z- (and M-) Estimation</h2>
<hr />
<p>
<strong>Proposition (asymptotic normality of m- and z-estimation under integrable envelope condition)</strong>. Let $\hat{\theta}_Z$ be the Z-estimator associated with $F_\theta(X)$ and suppose the following conditions hold.
</p>
<ol>
<li>There is a unique zero $\theta^* \in \{ \theta \in \Theta : \E[F_\theta(X)] = 0 \}$.
</li>
<li>The Z-estimator is consistent: $\|\hat{\theta}_Z - \theta^*\| = o_p(1)$.
</li>
<li>The covariance $\Cov[F_{\theta^*}(X)]$ is a bounded matrix.
</li>
<li>$\theta \mapsto F_\theta(X)$ is twice continuously differentiable for $P$-almost every $X$.
</li>
<li>$(\partial_\theta F_{\theta^*})(X) \not = 0$ for $P$-almost every $X$.
</li>
</ol>
<p>
Then,
$$
\sqrt{n}(\hat{\theta}_Z - \theta^*) \stackrel{d}{\longrightarrow} \Ncl(0, \Sigma) \qquad \Sigma = \E[D F_\theta(X)]^{-1} \ \Cov(F_{\theta^*}(X))\ \E[D F_\theta(X)]^{-1}
$$
where $DF_\theta(X)$ is the Jacobian (with respect to $\theta$).
</p>
<hr />
<p>
<em>Proof</em>. we will prove the 1-dimensional case, since the multivariate case just requires additional algebra. The main idea is to Taylor expand. There exists $\tilde{\theta} \in \{\lambda \hat{\theta}_Z + (1- \lambda) \theta^* : \lambda \in [0, 1] \}$ such that,
$$
\begin{aligned}
0 = \frac{1}{n}\sum_{i=1}^n F_{\hat{\theta}_Z}(X_i) &= \frac{1}{n}\sum_{i=1}^n \left( F_{\theta^*}(X_i) + (\partial_\theta F_{\theta^*}(X_i))(\hat{\theta}_Z-\theta^*)  + \frac{1}{2} (\partial_\theta^2 F_{\tilde{\theta}}(X_i)) (\hat{\theta}_Z - \theta^*)^2\right)
\end{aligned}
$$
so it follows
$$
\sqrt{n}(\hat{\theta}_Z - \theta^*) = - \left( \frac{1}{n}\sum_{i=1}^n (\partial_\theta F_{\theta^*}(X_i) )\right)^{-1} \left( \frac{1}{\sqrt{n}} \sum_{i=1}^n \left( F_{\theta^*}(X_i) + \frac{1}{2} (\partial_{\theta}F_{\tilde{\theta}})(X_i) (\hat{\theta}_Z - \theta)^2\right)\right).
$$
By the LLN and the continuous mapping theorem,
$$
\left( \frac{1}{n}\sum_{i=1}^n (\partial_\theta F_{\theta^*}(X_i) )\right)^{-1} \stackrel{d}{\longrightarrow}\E[\partial_\theta F_{\theta^*}(X)]^{-1}.
$$
Whereas by the CLT,
$$
\frac{1}{\sqrt{n}} \sum_{i=1}^n F_{\theta^*}(X_i) \stackrel{d}{\longrightarrow} \Ncl(0, \Var(F_{\theta^*}(X))).
$$
Similarly, by the CLT,
$$\frac{1}{2\sqrt{n}}\sum_{i=1}^n (\partial_{\theta^*} F_{\tilde{\theta}})(X_i) \stackrel{d}{\longrightarrow}\Ncl(0, \Var(\partial_\theta F_{\tilde{\theta}}(X))) = O_p(1)$$
and so it follows by Slutsky's theorem that
$$
\underbrace{\left[\frac{1}{2\sqrt{n}}\sum_{i=1}^n (\partial_{\theta^*} F_{\tilde{\theta}})(X_i) \right]}_{O_p(1)} \cdot \underbrace{\left[ (\hat{\theta}_Z - \theta^*)^2  \right]}_{o_p(1)} \stackrel{p}{\longrightarrow} 0.
$$
<span class="qed" />
</p>
            </div>
        </div>
        <div class="row">
        <p style="text-align: right; width:100%"><i><a href="../miscellany.html">Back...</a></i></p>
        </div>
    </body>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
</html>
