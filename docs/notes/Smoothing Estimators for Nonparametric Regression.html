
<!DOCTYPE html>
    <head>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="../styles/styles.css">
        <link rel="icon" href="assets/icon.ico" />
        <meta name="google-site-verification" content="gGsyj98Hmr9mZj0-DQzJIAdOP3eJXZJZnM4_NF8Mai8" />
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"> 
        <title>Mara Daniels - MIT</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                ],
                macros: {
                    '\\saeq': '\\{#1\\}',
                    '\\linf': '\\lim_{n \\to \\infty}',
                    '\\Alpha': '\\mathcal{A}',
                    '\\norm': '\\left\\lVert#1\\right\\rVert',
                    '\\ct': 'C^\\infty_c ( #1 )',
                    '\\lloc': 'L^1_{\\mathsf{loc}}( #1 )',
                    '\\Scr': '\\mathscr{S}',
                    '\\tmpr': '\\mathscr{S}',
                    '\\limnf': '\\liminf \\limits_{#1}',
                    '\\limsp': '\\limsup \\limits_{#1}',
                    '\\R': '{\\mathbb R}',
                    '\\C': '{\\mathbb C}',
                    '\\N': '{\\mathbb N}',
                    '\\Q': '{\\mathbb Q}',
                    '\\H': '{\\mathbb H}',
                    '\\S': '{\\mathbb S}',
                    '\\Z': '{\\mathbb Z}',
                    '\\E': '{\\mathbb E}',
                    '\\Var': '\\mathsf{Var}',
                    '\\Cov': '\\mathsf{Cov}',
                    '\\KL': '\\mathsf{KL}',
                    '\\Ent': '\\mathsf{Ent}',
                    '\\F': '{\\mathbb F}',
                    '\\Acl': '\\mathcal{A}',
                    '\\Bcl': '\\mathcal{B}',
                    '\\Ccl': '\\mathcal{C}',
                    '\\Dcl': '\\mathcal{D}',
                    '\\Ecl': '\\mathcal{E}',
                    '\\Fcl': '\\mathcal{F}',
                    '\\Gcl': '\\mathcal{G}',
                    '\\Hcl': '\\mathcal{H}',
                    '\\Jcl': '\\mathcal{J}',
                    '\\Kcl': '\\mathcal{K}',
                    '\\Lcl': '\\mathcal{L}',
                    '\\Mcl': '\\mathcal{M}',
                    '\\Ncl': '\\mathcal{N}',
                    '\\Ocl': '\\mathcal{O}',
                    '\\Pcl': '\\mathcal{P}',
                    '\\Qcl': '\\mathcal{Q}',
                    '\\Rcl': '\\mathcal{R}',
                    '\\Scl': '\\mathcal{S}',
                    '\\Tcl': '\\mathcal{T}',
                    '\\Ucl': '\\mathcal{U}',
                    '\\Vcl': '\\mathcal{V}',
                    '\\Wcl': '\\mathcal{W}',
                    '\\Xcl': '\\mathcal{X}',
                    '\\Ycl': '\\mathcal{Y}',
                    '\\Zcl': '\\mathcal{Z}',
                    '\\diag': '\\mathrm{diag}',
                    '\\supp': '\\mathrm{supp}',
                    '\\diam': '\\mathrm{diam}',
                    '\\sgn': '\\mathrm{sgn}',
                    '\\im': '\\mathrm{Im}',
                    '\\lcm': '\\mathrm{lcm}',
                    '\\aut': '\\mathrm{Aut}',
                    '\\inn': '\\mathrm{Inn}',
                    '\\rg': '\\mathrm{rg}',
                    '\\vol': '\\mathrm{vol}',
                    '\\Pr': '\\mathrm{Pr}',
                    '\\Tr': '\\mathrm{Tr}',
                    '\\eps': '\\varepsilon',
                    '\\charfct': '\\mathds{1}',
                    '\\nullfct': '{\\bf 0}',
                    '\\as': '\\text{a.s.}\\xspace',
                    '\\argmax': '\\mathop{\\mathrm{arg\\,max}}',
                    '\\argmin': '\\mathop{\\mathrm{arg\\,min}}',
                    '\\esssup': '\\mathop{\\mathrm{ess\\,sup}}',
                    '\\th': '\\hat{\\theta}',
                    '\\pto': '\\stackrel{p}{\\longrightarrow}',
                    '\\dto': '\\stackrel{d}{\\longrightarrow}',
                    '\\asto': '\\stackrel{a.s.}{\\longrightarrow}',
                    '\\coloneqq': ':='
                }
            });">
            </script>
    </head>
    <body>
        <div class="row">
            <div class="col-lg-12 col-xl-8 offset-xl-2"> 
            <h1>Smoothing Estimators for Nonparametric Regression</h1>
<p>
Consider a nonparametric regression problem with data of the form $\{(x_i, y_i)\}_{i=1}^n$ where
$$
y_i = f(x_i) + w_i
$$
and $f(x) : \R^d \to \R$ is a generic function of interest. The $w_i$, $i=1 \ldots n$ represent i.i.d. noise.
</p>
<p>
A <a href="Smoothing Estimators for Nonparametric Regression.html">smoothing estimator</a> is an estimation technique based on interpolating the observed data with a <em>smoothing kernel</em>, also known as a <em>Parzen window</em> or a <em>distance kernel</em> (not to be confused with an <a href="Reproducing Kernel Hilbert Spaces.html">RKHS</a> kernel). Correspondingly, this method is sometimes called the "Parzen window method" or "kernel regression" (not to be confused with <a href="Reproducing Kernel Ridge Regression.html">Kernel Ridge Regression</a>).
</p>
<hr />
<p>
<strong>Definition</strong>. A smoothing kernel is any integrable function $K(x): \R^d \to \R$, typically taking positive values and with decay at infinity.
</p>
<p>
The smoothing estimator $\hat{f}(x)$ is given by
$$
\hat{f}(x) = \frac{1}{n}\sum_{i=1}^n y_i \, K_h(x - x_i)
$$
where $K_h(x) = \frac{1}{h}K(x/h)$ is the rescaled kernel with <em>bandwidth</em> $h$.
</p>
<hr />
<p>
The parameter $h > 0$ can be tuned to improve the performance of the estimator. This is captured by the following theorem:
</p>
<hr />
<p>
<strong>Theorem</strong> (Bias-Variance Tradeoff in $d=1$). let $K(x)$ be a smoothing kernel. Assume that:
</p>
<ul>
<li>The kernel satisfies:
<ol>
<li>Symmetry: $K(x) = K(-x)$.
</li>
<li>Normalization: $\int K(x) \, dx = 1$.
</li>
<li>Integrability conditions:  $\int |x| K(x) \, dx < \infty$ and $\int K^2(x) P(z+hx) \, dx < \infty$ for any $z \in \R$.
</li>
</ol>
</li>
<li>The noise satisfies:
<ol>
<li>Zero mean: $\E[w_i \mid x_i = x] = 0$ for all $x \in \R$.
</li>
<li>Bounded variance: $\E[w_i^2 \mid x_i = x] \leq \sigma^2$ for all $x \in \R$.
</li>
</ol>
</li>
</ul>
<p>
Let $\bar{f}(x) = \E[\hat{f}(x)] = \E[f(x')K_h(x-x')]$ be the average estimator. If $f : \R \to \R$ is $L$-lipschitz and bounded by $B < \infty$, then the pointwise squared error at any $x \in \R$ is uniformly bounded:
$$
\begin{aligned}
\E_{\Scl}[|f(x) - \hat{f}(x)|^2] &= \underbrace{\E_{\Scl}[|f (x) - \bar{f}(x) |^2 ]}_{\text{squared bias}}+ \underbrace{\E_{\Scl}[|\bar{f}(x) - \hat{f}(x)|^2]}_{\text{variance}} \\
& \lesssim L^2 h^2 + \frac{B + \sigma^2}{nh} .
\end{aligned}
$$
where $\E_\Scl[\cdot]$ is an expectation over the sample. This bound is minimized when $h \sim n^{-1/3}$, in which case it decays at the rate $n^{-2/3}$.
</p>
<hr />
<p>
<em>Proof</em>. The key steps are to bound the bias and variance. Since the kernel integrates to one, the bias can be rewritten
$$
\begin{aligned}
|f(x) - \bar{f}(x)| & = \int(f(x) - f(x'))\frac{1}{h} K\left(\frac{x-x'}{h}\right) \, dx'
\end{aligned}
$$
and bounded by
$$
\begin{aligned}
|f(x) - \bar{f}(x)| & = \left| \int(f(x) - f(x'))\frac{1}{h} K\left(\frac{x-x'}{h}\right) \, dx' \right| \leq Lh \int \frac{|x-x'|}{h} K\left( \frac{x - x'}{h} \right) \frac{1}{h} \, dx' = C_1 Lh
\end{aligned}
$$
with $C_1 = \int |x| K(x) \, dx$.
</p>
<p>
The variance is bounded by
$$
\begin{aligned}
\E_\Scl[|\bar{f}(x) - \hat{f}(x) |^2] & = \E_\Scl \left[  \left| \frac{1}{n} \sum_{i=1}^n y_i K_h(x-x_i) - \bar{f}(x)\right|^2\right] \\
& \leq \frac{1}{n^2} \sum_{i=1}^n \E_{x_i, w_i}[|f(x_i)K_h(x - x_i)- \bar{f}(x) + w_i K_h(x - x_i) |^2 ] \\
& \leq \frac{1}{n} \E_{x'}[|f(x') K_h(x - x') - \bar{f}(x)|^2] + \frac{1}{n} \E_{x'}[w_i^2 K_h(x - x')^2]
\end{aligned}
$$
using the fact that $\E[w_i \mid x_i] = 0$ to cancel cross terms. The noise variance is bounded by,
$$
\begin{aligned}
\frac{1}{n}\E_{x'}[w_i^2 K_h(x - x')^2] & \leq \frac{\sigma^2}{n} \E_{x'}[K_h(x-x')^2] \\
& = \frac{\sigma^2}{nh} \int K(z)^2 P(x + hz) \, dz \\
& \leq \frac{\sigma^2 C_2}{nh}
\end{aligned}
$$
and the estimator variance is,
$$
\begin{aligned}
\frac{1}{n}\E_{x'}[|f(x')K_h(x-x') - \bar{f}(x)|^2] & \leq \frac{1}{n}\E_{x'}[|f(x')K_h(x-x')|^2]  \\
& \leq \frac{B^2}{nh} \int \frac{1}{h} K \left( \frac{x - x'}{h}\right)^2 P(x') \, dx' \\
& = \frac{B^2}{nh} \int K(z)^2 P(x +hz) \, dz \\
& \leq \frac{B^2 C_2}{nh}
\end{aligned}
$$
Leading to an overall bound by $C_2(\sigma^2 + B) / nh$.
</p>
<h3 id="variations-on-the-above-proof">Variations on the above proof</h3>
<h4 id="under-a-hölder-condition">Under a Hölder Condition</h4>
<p>
Suppose that $f$ satisfies a <a href="Smoothing Estimators for Nonparametric Regression.html#under-a-hölder-condition">Hölder Condition</a>: it is $k$-times differentiable and that its $k$-th derivative is bounded $\|f^{(k)}(x) \|_{\infty} \leq B$. Suppose further that:
$$
\int x^s K(x) \, dx = 0 \qquad 1 \leq s < k \qquad \qquad \qquad \int x^k K(x) \, dx < B
$$
such a kernel is known as an <a href="Smoothing Estimators for Nonparametric Regression.html#under-a-sobolev-condition">k-th order smoothing kernel</a>. To analyze the bias:
$$
\begin{aligned}
|f(x) - \bar{f}(x)| & = \left| \int(f(x) - f(x'))\frac{1}{h} K\left(\frac{x-x'}{h}\right) \, dx'\right|  = \left| \int(f(x) - f(x + hz)) K(z) \, dz \right|.
\end{aligned}
$$
By Taylor expansion, there exists $x_r \in \{x + tz : 0 < t < h\}$ such that
$$
f(x+hz) - f(x) = \sum_{s=1}^{k-1} f^{(s)}(x)h^s z^s + f^{(k)}(x_r)h^k z^k
$$
and so
$$
|f(x) - \bar{f}(x)| \leq \sum_{s=1}^{k-1} h^s f^{(s)}(x) \left| \int z^s K(z)  \, dz\right| + h^k \left|\int f^{(k)}(x_r) z^k K(z)\, dz\right| \leq h^k B^2.
$$
In this case, the optimal bandwidth becomes $h \sim n^{-\frac{1}{2k+1}}$, which leads to error decay like $n^{-\frac{2k}{2k+1}}$.
</p>
<h4 id="in-higher-dimensions">In Higher Dimensions</h4>
<p>
Increasing the dimension changes the $u$-substitutions in the above analysis, leading to different rates. By the same arguments,
$$
\begin{aligned}
|f(x) - \bar{f}(x)|^2 & \leq C_1^2 L^2 h^{2d} \qquad \E_\Scl[|\hat{f}(x) - \bar{f}(x)|^2]  \leq \frac{C_2(\sigma^2 + B^2)}{nh^{2-d}}
\end{aligned}
$$
These terms are balanced when $h \sim n^{-\frac{1}{2+d}}$ leading to the rate
$$
\E_{\Scl}[|f(x) - \hat{f}(x)|^2] \lesssim n^{-\frac{2}{2+d}}.
$$
</p>
<h4 id="with-a-bounded-kernel">With a Bounded Kernel</h4>
<p>
Suppose $\|K(\cdot)\|_{\infty} \leq B$ also. Then in the above variance bounds,  $$ C_2 = \int K(z)^2 P(x + hz) \, dz \leq B^2 \int P(x + hz) \, dz \leq \frac{B^2}{h^d}$$which leads to the bias and variance bounds
$$
\begin{aligned}
|f(x) - \bar{f}(x)|^2 & \leq C_1^2 L^2 h^{2d} \qquad \E_\Scl[|\hat{f}(x) - \bar{f}(x)|^2]  \leq \frac{B^2(\sigma^2 + B^2)}{nh^{3-d}} .
\end{aligned}
$$
These terms are balanced when $h \sim n^{-\frac{1}{2(d+1)}}$ leading to the rate
$$
\E_{\Scl}[|f(x) - \hat{f}(x)|^2] \leq n^{-\frac{2}{2+2d}}$$
Basically, we can weaken the condition $\int K(z)^2 P(x+hz) \, dz < \infty$ to the condition $\|K\|_{\infty}< \infty$ and pay a similar rate, but with dimension doubled.
</p>
<h4 id="under-a-poincaré-inequality">Under a Poincaré Inequality</h4>
<p>
<ins>Interesting question: why can't I improve the variance bound using a concentration inequality? This is pretty intuitive, but it doesn't work! Whereas it is straightforward to improve the bias inequality using higher order smoothness...</ins>
</p>
<p>
If $P(x)$ satisfies a Poincaré inequality of the form $\mathsf{Var}[f(x)] \leq C_{\mathsf{PI}} \E[|\nabla f\|^2]$ then the analysis of the variance bound becomes,
$$
\begin{align*}
\E<em>{x'}[|f(x') K</em>h(x - x') - \bar{f}(x)|^2] & \leq C<em>{\mathsf{PI}} \E</em>{x'}\left[\left\|\nabla f(x') K_h(x-x')\ - \frac{1}{h^2}f(x') (\nabla K) \left( \frac{x-x'}{h}\right) \right\|^2 \right] \\
& \leq \frac{C<em>{\mathsf{PI}} C</em>2 L^2}{h} \cdot \frac{1}{h^{1-d}} + \frac{C_{\mathsf{PI}} B^2}{h^3} \cdot \frac{1}{h^{1-d}} \cdot \int \|\nabla K(z)\|^2 P(x + hz) \, dz
\end{align*}
</p>
            </div>
        </div>
        <div class="row">
        <p style="text-align: right; width:100%"><i><a href="../miscellany.html">Back...</a></i></p>
        </div>
    </body>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
</html>
