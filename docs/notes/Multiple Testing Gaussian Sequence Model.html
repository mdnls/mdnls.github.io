
<!DOCTYPE html>
    <head>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="../styles/styles.css">
        <link rel="icon" href="assets/icon.ico" />
        <meta name="google-site-verification" content="gGsyj98Hmr9mZj0-DQzJIAdOP3eJXZJZnM4_NF8Mai8" />
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"> 
        <title>Mara Daniels - MIT</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                ],
                macros: {
                    '\\saeq': '\\{#1\\}',
                    '\\linf': '\\lim_{n \\to \\infty}',
                    '\\Alpha': '\\mathcal{A}',
                    '\\norm': '\\left\\lVert#1\\right\\rVert',
                    '\\ct': 'C^\\infty_c ( #1 )',
                    '\\lloc': 'L^1_{\\mathsf{loc}}( #1 )',
                    '\\Scr': '\\mathscr{S}',
                    '\\tmpr': '\\mathscr{S}',
                    '\\limnf': '\\liminf \\limits_{#1}',
                    '\\limsp': '\\limsup \\limits_{#1}',
                    '\\R': '{\\mathbb R}',
                    '\\C': '{\\mathbb C}',
                    '\\N': '{\\mathbb N}',
                    '\\Q': '{\\mathbb Q}',
                    '\\H': '{\\mathbb H}',
                    '\\S': '{\\mathbb S}',
                    '\\Z': '{\\mathbb Z}',
                    '\\E': '{\\mathbb E}',
                    '\\Var': '\\mathsf{Var}',
                    '\\Cov': '\\mathsf{Cov}',
                    '\\KL': '\\mathsf{KL}',
                    '\\Ent': '\\mathsf{Ent}',
                    '\\F': '{\\mathbb F}',
                    '\\Acl': '\\mathcal{A}',
                    '\\Bcl': '\\mathcal{B}',
                    '\\Ccl': '\\mathcal{C}',
                    '\\Dcl': '\\mathcal{D}',
                    '\\Ecl': '\\mathcal{E}',
                    '\\Fcl': '\\mathcal{F}',
                    '\\Gcl': '\\mathcal{G}',
                    '\\Hcl': '\\mathcal{H}',
                    '\\Jcl': '\\mathcal{J}',
                    '\\Kcl': '\\mathcal{K}',
                    '\\Lcl': '\\mathcal{L}',
                    '\\Mcl': '\\mathcal{M}',
                    '\\Ncl': '\\mathcal{N}',
                    '\\Ocl': '\\mathcal{O}',
                    '\\Pcl': '\\mathcal{P}',
                    '\\Qcl': '\\mathcal{Q}',
                    '\\Rcl': '\\mathcal{R}',
                    '\\Scl': '\\mathcal{S}',
                    '\\Tcl': '\\mathcal{T}',
                    '\\Ucl': '\\mathcal{U}',
                    '\\Vcl': '\\mathcal{V}',
                    '\\Wcl': '\\mathcal{W}',
                    '\\Xcl': '\\mathcal{X}',
                    '\\Ycl': '\\mathcal{Y}',
                    '\\Zcl': '\\mathcal{Z}',
                    '\\diag': '\\mathrm{diag}',
                    '\\supp': '\\mathrm{supp}',
                    '\\diam': '\\mathrm{diam}',
                    '\\sgn': '\\mathrm{sgn}',
                    '\\im': '\\mathrm{Im}',
                    '\\lcm': '\\mathrm{lcm}',
                    '\\aut': '\\mathrm{Aut}',
                    '\\inn': '\\mathrm{Inn}',
                    '\\rg': '\\mathrm{rg}',
                    '\\vol': '\\mathrm{vol}',
                    '\\Pr': '\\mathrm{Pr}',
                    '\\Tr': '\\mathrm{Tr}',
                    '\\eps': '\\varepsilon',
                    '\\charfct': '\\mathds{1}',
                    '\\nullfct': '{\\bf 0}',
                    '\\as': '\\text{a.s.}\\xspace',
                    '\\argmax': '\\mathop{\\mathrm{arg\\,max}}',
                    '\\argmin': '\\mathop{\\mathrm{arg\\,min}}',
                    '\\esssup': '\\mathop{\\mathrm{ess\\,sup}}',
                    '\\th': '\\hat{\\theta}',
                    '\\pto': '\\stackrel{p}{\\longrightarrow}',
                    '\\dto': '\\stackrel{d}{\\longrightarrow}',
                    '\\asto': '\\stackrel{a.s.}{\\longrightarrow}',
                    '\\coloneqq': ':='
                }
            });">
            </script>
    </head>
    <body>
        <div class="row">
            <div class="col-lg-12 col-xl-8 offset-xl-2"> 
            <h1>Multiple Testing Gaussian Sequence Model</h1>
<hr />
<p>
<strong>Definition</strong>. consider a multiple hypothesis testing problem with observations $Y_i = \mu_i + z_i$ with $z_i \sim \Ncl(0, 1)$, null hypotheses $H_{0, i} : \mu_i = 0$ and alternatives $H_{1, i} : \mu_i \not = 0$.
</p>
<hr />
<h2 id="sparse-testing">Sparse Testing</h2>
<p>
In the sparse testing setting, we assume that at most one of the $H_{1, i}$ is non-null.
</p>
<h3 id="lower-bound-via-reduction-to-simple-testing">Lower bound via reduction to simple testing</h3>
<hr />
<p>
<strong>Proposition</strong>. Consider the alternatives $H_{1, i} : 0 < \mu_i < (1-\eps) \sqrt{2 \log n}$ and $\mu_j = 0$ for $j \not = i$. In this setting, no test $\Psi(Y)$ can attain
$$
P_0[\Psi\text{ rejects global null}] + \sup_{1 \leq i \leq n} P_{1, i}[\Psi\text{ accepts global null}] < 1.
$$
<em>*</em>
</p>
<p>
The intuition behind this proof is that, on $H_{1, i}$, it will hold that $\max_{j \not = i} Y_j = \sqrt{2 \log n} + o(1)$ and that (with high probability) $Y_i < \sqrt{2 \log n}$. It is therefore impossible to detect that $\mu_i \not = 0$.
</p>
<p>
<em>Proof sketch</em>: by a standard reduction to Bayesian testing:
</p>
<ol>
<li>For any distribution $\pi$ over $i \in [n]$, we have $\sup_{1 \leq i \leq n} P_{1, i}[\Psi \text{ rejects the global null}] \geq \E_{\pi} P_{1, i}[\Psi \text{ rejects the global null}]$, and the latter quantity is the Type II error of a simple test between $H_0 = \cap_{1 \leq i \leq n} H_{0, i}$ and $H_1 : Y \sim \E_{\pi}[P_{1, i}(Y)]$.
</li>
<li>For any $0 < \alpha < 1$, by <a href="Optimality of Likelihood Ratio Testing.html">Optimality of Likelihood Ratio Testing</a>, we can compute the best power of any estimator with Type I error at most $\alpha$ on the simple test.
</li>
<li>Set $\pi = \mathsf{Unif}([n])$ set the threshold $\mu = (1 - \eps)\sqrt{2\log n}$. Let $H_1$ be the hypothesis that $\mu_i = \mu$ for $i \sim \pi$, and $\mu_j = 0$ otherwise. The likelihood ratio $P_1(Y)/P_0(Y)$ can be written
</li>
</ol>
<p>
$$
L_n(Y) = \frac{1}{n}\sum_{i=1}^n e^{Y_i \mu_i - \mu_i^2/2}.
$$
</p>
<ol>
<li>If $\mu_i$ did not depend on $n$, then we could approximate $L_n(Y)$ with LLN/CLT, but because of the dependence these theorems don't apply out of the box. However one can show $L_n(Y) \stackrel{p}{\longrightarrow}\E_0[L_n(Y)] = 1$ when $\mu = (1 - \eps) \sqrt{2 \log n}$ using a truncation argument (specifically, analyzing instead $\hat{L}_n = L_n(Y)1\{\max Y_i \leq \sqrt{2 \log n} \})$, which has $L_n = \hat{L}_n + o_p(1)$ and $\E[\hat{L}_n] \to 1$).
</li>
</ol>
<h3 id="upper-bound-via-bonferronis-method">Upper bound via Bonferroni's method</h3>
<hr />
<p>
<strong>Proposition</strong>. Consider the alternatives $H_{1, i}$ :  $\mu_i > (1+\eps)\sqrt{2 \log n}$ and $\mu_j = 0$ for $j \not = i$.  In this case, for any $0 < \alpha < 1$, Bonferroni's method with threshold $\alpha$ has global error at most $\alpha$ and power which approaches $1$ as $n \to \infty$.
</p>
<hr />
<p>
<em>Proof sketch</em>:
</p>
<ol>
<li>In this setting, Bonferroni's method is equivalent to computing $$ T_n  = \min_{1 \leq i \leq n}Y_i$$ and rejecting the null hypothesis whenever $T_n \leq \Phi^{-1}(\alpha /n)$ where $\Phi^{-1}(x)$ is inverse of the Gaussian cdf. This quantity is called the $\alpha/n$-percentile. Alternatively, we could compute $T_n = \max_{1 \leq i \leq n}Y_i$ and compare against $z(\alpha/n) = (1 - \Phi)^{-1}(\alpha/n)$, which is called the $z$-score. We will use the latter notation, which seems to be more popular in the literature.
</li>
<li>Using standard <a href="https://en.wikipedia.org/wiki/Q-function#Bounds<em>and</em>approximations">Gaussian tail bounds</a> it is easy to prove $z(\alpha / n) \approx \sqrt{2 \log n} + o(1)$, where $\alpha$ appears in the lower order constants.
<ul>
<li>Intuitively, since Bonferroni's method has Type 1 error close to $\alpha$, we know the events $T_n \geq z(\alpha / n)$ and $T_n \leq z(\alpha / n)$ both occur a positive fraction of the time under the null hypothesis.
</li>
<li>But it is a standard fact that $T_n / \sqrt{2 \log n} \stackrel{p}{\longrightarrow} 1$, so $z(\alpha / n) \to \sqrt{2 \log n}$ is required.
</li>
</ul>
</li>
<li>This immediately implies that if $\mu_i > (1 + \eps)\sqrt{2 \log n}$ then as $n \to \infty$, $P[T_n \geq z(\alpha/n)] \to 1$.
</li>
<li>We can also observe that the lower bound is in effect. Roughly speaking, if $\mu_i < \sqrt{2 \log n}$ for all $i = 1\ldots n$ then $Y_i \geq \max_{j \not = i} Y_j$ has negligible probability, so that
</li>
</ol>
<p>
$$
P(\max_{j \not = i} Y_i > z(\alpha/n)) = 1 - P(Z < z(\alpha/n))^n = 1 - \left(1 - \frac{\alpha}{n}\right)^n \approx 1 - e^{-\alpha} \leq 1 - \alpha.
$$
where $Z \sim \Ncl(0, 1)$. Also, note that $1 - (1 - \alpha/n)^n \leq 1 - \alpha$ is always true, but these approximations become tight as $\alpha \to 0$.
</p>
<h2 id="distributed-testing">Distributed Testing</h2>
<h3 id="lower-bound-via-reduction-to-simple-testing">Lower bound via reduction to simple testing</h3>
<p>
By the same argument as the lower bound for sparse testing, it's sufficient to analyze the simple testing problem
$$
\begin{aligned}
& H_0 : \mu = 0, \ Y \sim \Ncl(0, I_n) \\
& H_1 : \mu \sim \pi_r^{(n)}, \ Y \sim \Ncl (\mu, I_n)
\end{aligned}
$$
where $\pi_r^{(n)}$ is the uniform measure on $r S^{(n-1)}$. In this setting, the likelihood ratio test is optimal, and the likelihood ratio is given by:
$$
L_n(Y) \coloneqq \frac{P_1(Y)}{P_0(Y)} = e^{\frac{1}{2}\|y\|^2}\int_{rS^{n-1}} e^{-\frac{1}{2}\|y-\mu\|^2} \, dS(\mu) = e^{-r^2/2} \int_{rS^{n-1}} e^{\langle Y, \mu\rangle} \, dS(\mu)
$$
<span style="color:red">Apparently, the likelihood ratio converges to 1 whenever</span> $r^2/\sqrt{2n}\longrightarrow 0$.
</p>
<h3 id="upper-bound-via-anova">Upper bound via ANOVA</h3>
<p>
The ANOVA test is based on  ...
</p>
            </div>
        </div>
        <div class="row">
        <p style="text-align: right; width:100%"><i><a href="../miscellany.html">Back...</a></i></p>
        </div>
    </body>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
</html>
