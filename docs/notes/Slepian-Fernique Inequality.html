
<!DOCTYPE html>
    <head>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="../styles/styles.css">
        <link rel="icon" href="assets/icon.ico" />
        <meta name="google-site-verification" content="gGsyj98Hmr9mZj0-DQzJIAdOP3eJXZJZnM4_NF8Mai8" />
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"> 
        <title>Mara Daniels - MIT</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                ],
                macros: {
                    '\\saeq': '\\{#1\\}',
                    '\\linf': '\\lim_{n \\to \\infty}',
                    '\\Alpha': '\\mathcal{A}',
                    '\\norm': '\\left\\lVert#1\\right\\rVert',
                    '\\ct': 'C^\\infty_c ( #1 )',
                    '\\lloc': 'L^1_{\\mathsf{loc}}( #1 )',
                    '\\Scr': '\\mathscr{S}',
                    '\\tmpr': '\\mathscr{S}',
                    '\\limnf': '\\liminf \\limits_{#1}',
                    '\\limsp': '\\limsup \\limits_{#1}',
                    '\\R': '{\\mathbb R}',
                    '\\C': '{\\mathbb C}',
                    '\\N': '{\\mathbb N}',
                    '\\Q': '{\\mathbb Q}',
                    '\\H': '{\\mathbb H}',
                    '\\S': '{\\mathbb S}',
                    '\\Z': '{\\mathbb Z}',
                    '\\E': '{\\mathbb E}',
                    '\\Var': '\\mathsf{Var}',
                    '\\Cov': '\\mathsf{Cov}',
                    '\\KL': '\\mathsf{KL}',
                    '\\Ent': '\\mathsf{Ent}',
                    '\\F': '{\\mathbb F}',
                    '\\Acl': '\\mathcal{A}',
                    '\\Bcl': '\\mathcal{B}',
                    '\\Ccl': '\\mathcal{C}',
                    '\\Dcl': '\\mathcal{D}',
                    '\\Ecl': '\\mathcal{E}',
                    '\\Fcl': '\\mathcal{F}',
                    '\\Gcl': '\\mathcal{G}',
                    '\\Hcl': '\\mathcal{H}',
                    '\\Jcl': '\\mathcal{J}',
                    '\\Kcl': '\\mathcal{K}',
                    '\\Lcl': '\\mathcal{L}',
                    '\\Mcl': '\\mathcal{M}',
                    '\\Ncl': '\\mathcal{N}',
                    '\\Ocl': '\\mathcal{O}',
                    '\\Pcl': '\\mathcal{P}',
                    '\\Qcl': '\\mathcal{Q}',
                    '\\Rcl': '\\mathcal{R}',
                    '\\Scl': '\\mathcal{S}',
                    '\\Tcl': '\\mathcal{T}',
                    '\\Ucl': '\\mathcal{U}',
                    '\\Vcl': '\\mathcal{V}',
                    '\\Wcl': '\\mathcal{W}',
                    '\\Xcl': '\\mathcal{X}',
                    '\\Ycl': '\\mathcal{Y}',
                    '\\Zcl': '\\mathcal{Z}',
                    '\\diag': '\\mathrm{diag}',
                    '\\supp': '\\mathrm{supp}',
                    '\\diam': '\\mathrm{diam}',
                    '\\sgn': '\\mathrm{sgn}',
                    '\\im': '\\mathrm{Im}',
                    '\\lcm': '\\mathrm{lcm}',
                    '\\aut': '\\mathrm{Aut}',
                    '\\inn': '\\mathrm{Inn}',
                    '\\rg': '\\mathrm{rg}',
                    '\\vol': '\\mathrm{vol}',
                    '\\Pr': '\\mathrm{Pr}',
                    '\\Tr': '\\mathrm{Tr}',
                    '\\eps': '\\varepsilon',
                    '\\charfct': '\\mathds{1}',
                    '\\nullfct': '{\\bf 0}',
                    '\\as': '\\text{a.s.}\\xspace',
                    '\\argmax': '\\mathop{\\mathrm{arg\\,max}}',
                    '\\argmin': '\\mathop{\\mathrm{arg\\,min}}',
                    '\\esssup': '\\mathop{\\mathrm{ess\\,sup}}',
                    '\\th': '\\hat{\\theta}',
                    '\\pto': '\\stackrel{p}{\\longrightarrow}',
                    '\\dto': '\\stackrel{d}{\\longrightarrow}',
                    '\\asto': '\\stackrel{a.s.}{\\longrightarrow}',
                    '\\coloneqq': ':='
                }
            });">
            </script>
    </head>
    <body>
        <div class="row">
            <div class="col-lg-12 col-xl-8 offset-xl-2"> 
            <h1>Slepian-Fernique Inequality</h1>
<hr />
<p>
<strong>Proposition</strong>: let $X \sim \Ncl(0, \Sigma_X)$ and $Y \sim \Ncl(0, \Sigma_Y)$ be $n$-dimensional Gaussian vectors. Suppose
$$
\E[|X_i - X_j|^2] \leq \E[|Y_i - Y_j|^2] \qquad \text{for all } i,j= 1,\ldots,n
$$
Then $\E[\max_{i = 1\ldots n} X_i] \leq \E[\max_{i=1\ldots n}Y_i]$.
</p>
<hr />
<p>
<em>Proof</em>. We will apply <a href="Gaussian Interpolation.html">Gaussian Interpolation</a> to $X_t = \sqrt{t}Y + \sqrt{1-t}X$. Also, set $S_\beta(X) = \frac{1}{\beta} \log \sum_{i=1}^n e^{\beta X_i}$ the softmax function. Then,
$$
\partial_{i} S_\beta(X) = \frac{ e^{\beta X_i}}{\sum_{i=1}^n e^{\beta X_i}} =: p_i(X)
$$
and similarly
$$
\partial_{ij} S_\beta(X) = \beta(\delta_{i=j} \, p_i(X) - p_i(X) p_j(X))
$$
and so it follows that
$$
\partial_t [S_\beta(X_t)] = \partial_t\E[\langle \Sigma_Y - \Sigma_X, \nabla^2S_\beta(X_t) \rangle_F].
$$
Note that since $p_i(X) = 1 - \sum_{j \not = i} p_j(X)$, we have that for any $a \in \R^n$,
$$
\langle \diag(a), \nabla^2S_\beta(X)\rangle_F =\sum_{i=1}^n a_i p_i(X)(1-p_i(X)) = \sum_{i \not = j} a_i p_i(X) p_j(X) = \frac{1}{2}\sum_{i\not = j}^n (a_i p_i(X) p_j(X) + a_jp_j(X) p_i(X))
$$
What this implies is that for any symmetric matrix $\Sigma$,
$$
\langle \Sigma, \nabla^2 S_\beta(X) \rangle = \frac{1}{2} \sum_{i \not = j} p_i(X) p_j(X) \left[ \Sigma_{ii} + \Sigma_{jj} - 2\Sigma_{ij}\right]
$$
and in particular, plugging this into the above inequality yields
$$
\partial_t \E[\langle \Sigma_Y - \Sigma_X, \nabla^2 S_\beta(X_t)\rangle_F] = \frac{1}{2}\sum_{i\not=j} [\E[|Y_i - Y_j|^2] - \E[|X_i - X_j|^2]] \, p_i(X) p_j(X) \geq 0
$$
<span class="qed" />
</p>
<p>
As a corollary, note that the claim of the proposition is also implied by the weaker conditions $\E[X_i^2] = \E[Y_i^2]$ and $\E[Y_i Y_j] \leq \E[X_i X_j]$ for all $i, j = 1 \ldots n$. This inequality is easier to prove directly and it is sometimes called 'Slepian's Inequality.'
</p>
<p>
<del>There's something extremely interesting about how the hessian of the softmax function appears in this proof and plays a role similar to a riemannian metric...</del>
</p>
            </div>
        </div>
        <div class="row">
        <p style="text-align: right; width:100%"><i><a href="../miscellany.html">Back...</a></i></p>
        </div>
    </body>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
</html>
