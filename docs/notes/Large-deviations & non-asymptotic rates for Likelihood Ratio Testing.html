
<!DOCTYPE html>
    <head>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="../styles/styles.css">
        <link rel="icon" href="assets/icon.ico" />
        <meta name="google-site-verification" content="gGsyj98Hmr9mZj0-DQzJIAdOP3eJXZJZnM4_NF8Mai8" />
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"> 
        <title>Mara Daniels - MIT</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                ],
                macros: {
                    '\\saeq': '\\{#1\\}',
                    '\\linf': '\\lim_{n \\to \\infty}',
                    '\\Alpha': '\\mathcal{A}',
                    '\\norm': '\\left\\lVert#1\\right\\rVert',
                    '\\ct': 'C^\\infty_c ( #1 )',
                    '\\lloc': 'L^1_{\\mathsf{loc}}( #1 )',
                    '\\Scr': '\\mathscr{S}',
                    '\\tmpr': '\\mathscr{S}',
                    '\\limnf': '\\liminf \\limits_{#1}',
                    '\\limsp': '\\limsup \\limits_{#1}',
                    '\\R': '{\\mathbb R}',
                    '\\C': '{\\mathbb C}',
                    '\\N': '{\\mathbb N}',
                    '\\Q': '{\\mathbb Q}',
                    '\\H': '{\\mathbb H}',
                    '\\S': '{\\mathbb S}',
                    '\\Z': '{\\mathbb Z}',
                    '\\E': '{\\mathbb E}',
                    '\\Var': '\\mathsf{Var}',
                    '\\Cov': '\\mathsf{Cov}',
                    '\\KL': '\\mathsf{KL}',
                    '\\Ent': '\\mathsf{Ent}',
                    '\\F': '{\\mathbb F}',
                    '\\Acl': '\\mathcal{A}',
                    '\\Bcl': '\\mathcal{B}',
                    '\\Ccl': '\\mathcal{C}',
                    '\\Dcl': '\\mathcal{D}',
                    '\\Ecl': '\\mathcal{E}',
                    '\\Fcl': '\\mathcal{F}',
                    '\\Gcl': '\\mathcal{G}',
                    '\\Hcl': '\\mathcal{H}',
                    '\\Jcl': '\\mathcal{J}',
                    '\\Kcl': '\\mathcal{K}',
                    '\\Lcl': '\\mathcal{L}',
                    '\\Mcl': '\\mathcal{M}',
                    '\\Ncl': '\\mathcal{N}',
                    '\\Ocl': '\\mathcal{O}',
                    '\\Pcl': '\\mathcal{P}',
                    '\\Qcl': '\\mathcal{Q}',
                    '\\Rcl': '\\mathcal{R}',
                    '\\Scl': '\\mathcal{S}',
                    '\\Tcl': '\\mathcal{T}',
                    '\\Ucl': '\\mathcal{U}',
                    '\\Vcl': '\\mathcal{V}',
                    '\\Wcl': '\\mathcal{W}',
                    '\\Xcl': '\\mathcal{X}',
                    '\\Ycl': '\\mathcal{Y}',
                    '\\Zcl': '\\mathcal{Z}',
                    '\\diag': '\\mathrm{diag}',
                    '\\supp': '\\mathrm{supp}',
                    '\\diam': '\\mathrm{diam}',
                    '\\sgn': '\\mathrm{sgn}',
                    '\\im': '\\mathrm{Im}',
                    '\\lcm': '\\mathrm{lcm}',
                    '\\aut': '\\mathrm{Aut}',
                    '\\inn': '\\mathrm{Inn}',
                    '\\rg': '\\mathrm{rg}',
                    '\\vol': '\\mathrm{vol}',
                    '\\Pr': '\\mathrm{Pr}',
                    '\\Tr': '\\mathrm{Tr}',
                    '\\eps': '\\varepsilon',
                    '\\charfct': '\\mathds{1}',
                    '\\nullfct': '{\\bf 0}',
                    '\\as': '\\text{a.s.}\\xspace',
                    '\\argmax': '\\mathop{\\mathrm{arg\\,max}}',
                    '\\argmin': '\\mathop{\\mathrm{arg\\,min}}',
                    '\\esssup': '\\mathop{\\mathrm{ess\\,sup}}',
                    '\\th': '\\hat{\\theta}',
                    '\\pto': '\\stackrel{p}{\\longrightarrow}',
                    '\\dto': '\\stackrel{d}{\\longrightarrow}',
                    '\\asto': '\\stackrel{a.s.}{\\longrightarrow}',
                    '\\coloneqq': ':='
                }
            });">
            </script>
    </head>
    <body>
        <div class="row">
            <div class="col-lg-12 col-xl-8 offset-xl-2"> 
            <h1>Large-deviations & non-asymptotic rates for Likelihood Ratio Testing</h1>
<p>
The <a href="Likelihood Ratio Test.html">likelihood ratio test</a> is an optimal test (with respect to <a href="Hypothesis Testing.html">weighted error probability</a>) for distinguishing two univariate distributions. Intuitively, when we have multiple samples, it should become easier to distinguish between the null and alternative. Using <a href="Large Deviations.html">large-deviation theory</a>, we can quantitatively bound the decay rate of Type I and II error probabilities given access to $n$ samples.
</p>
<p>
<strong>Intuition</strong>: let $Y^n = (Y_1 \ldots Y_n)$ i.i.d. be real-valued samples. The <a href="Likelihood Ratio Test.html">likelihood ratio</a> $L_n(Y^n)$ has,
$$
\begin{aligned}
\E_1[L_n(Y^n)] & = \frac{1}{n}\sum_{i=1}^n \E_{Y_i \sim P_1}\left[\log \left( \frac{P_1(Y_i)}{P_0(Y_i)}\right)\right] = \KL(P_1 \mid P_0)\\
\E_0[L_n(Y^n)] & = \frac{1}{n}\sum_{i=1}^n \E_{Y_i \sim P_0}\left[\log \left( \frac{P_1(Y_i)}{P_0(Y_i)}\right)\right] = -\KL(P_0 \mid P_1)
\end{aligned}
$$
Therefore, if we choose a threshold $t \in [-\KL(P_0 \mid P_1), \KL(P_1 \mid P_0)]$, then we can combine an upper tail bound (for $p_0$) and a lower tail bound (for $p_1$) to control the probabilities of false positive/T1 and false negative/T2 errors.
$$
\begin{aligned}
P_0(\Psi_t(Y^n) = 1)& =P_0(L_n(Y^n) - \E_0[L_n(Y^n)] > s_0)  & s_0 = t - \E_0[L_n(Y^n)] > 0 \\
P_1(\Psi_t(Y^n) = 0)& = P_1( L_n(Y^n) - \E_1[L_n(Y^n)]  < -s_1) & s_1 = \E_1[L_n(Y^n)] - t> 0
\end{aligned}
$$
The relevant quantity for an upper tail bound of $P_0$ is the log-MGF of $L_n(Y^n)$, which we will call $\Phi_n(\lambda)$. It has an interesting form:
$$
\begin{aligned}
\frac{1}{n}\Phi_n(n \lambda) & = \frac{1}{n}\log \E_0[\exp(n \lambda L_n(Y^n))] \\
&=\frac{1}{n}\log \E_0\left[ \prod_{i=1}^n \exp \left( \lambda \log \frac{p_1(Y_i)}{p_0(Y_i)}\right) \right] \\
& = \log \int p_1(y)^\lambda p_0(y)^{1-\lambda} \, dy \\
& =: \Phi(\lambda)
\end{aligned}
$$
First, note that the rescaling $\Phi_n(\lambda) \to n^{-1} \Phi_n(n \lambda)$ is chosen for convenience, since it makes the right hand side invariant to sample size.  What is interesting is that, for $\lambda \in [0, 1]$, the integrand is an exponential interpolation between $p_0(y)$ and $p_1(y)$. In fact, it can be viewed as the <a href="Exponential Families.html">log partition function</a> of an exponential family $(p_\lambda)_{\lambda \in [0, 1]}$ that interpolates $p_0$, $p_1$.
</p>
<hr />
<p>
<strong>Definition (Interpolating Exponential Family)</strong>: for $\lambda \in [0, 1]$, define the exponential family
$$
p_\lambda(y) = p_0(y)\exp\left(\lambda \log \frac{p_1(y)}{p_0(y)} - \Phi(\lambda)\right)
$$
with the log-normalization function
$$
\begin{aligned}
\Phi(y) & = \log \int \exp \left( \lambda \log \frac{p_1(y)}{p_0(y)}\right) p_0(y) \, dy = \log \int p_1(y)^\lambda p_0(y)^{1-\lambda}\, dy.
\end{aligned}
$$
</p>
<hr />
<p>
We can immediately derive a tail bound for the Type I error using Chernoff's bound:
$$
\begin{aligned}
P_0(\Phi_t = 1) & \leq \inf_{\lambda \geq 0} \exp ( \Phi_n(\lambda) - \lambda t) \\
& = \inf_{\lambda \geq 0} \exp( n \Phi(\lambda/n) - \lambda t) \\
& = \exp( -n \Phi^*(t))
\end{aligned}
$$
where we used the property that $a f(x)$ is (convex) dual to $a f^*(x/a)$ for generic convex $f : \R \to \R$. The quantity $\Phi^*(t)$ is the <em>large deviations rate parameter</em>, and because it is also the log-normalization of the interpolating exponential family, we know (eg. by Donsker-Varadhan) that it can be written $\Phi^*(t) = \KL(p_{\lambda^*(t)} \mid p_0)$ where $\lambda^*(t)$ attains the suprema. It turns out this tail bound also applies to the false negative / Type II error rate, which is the contents of the next theorem.
</p>
<hr />
<p>
<strong>Theorem (large deviations for simple testing)</strong>: the interpolating exponential family is <a href="Exponential Families.html">minimal</a> and therefore the mapping $t \mapsto \lambda^*(t) \coloneqq \partial_t \Phi^*(t)$ is $C^1(\R)$. The Type I and Type II errors have rate functions given by,
$$
\begin{aligned}
P_0(\Psi_t = 1) \leq e^{-n \KL(p_{\lambda^*} \mid p_0)} \qquad P_1(\Psi_t = 0) \leq e^{-n \KL(p_{\lambda^*} \mid p_1)}.
\end{aligned}
$$
</p>
<hr />
<p>
<em>Proof sketch</em>. the exponential family is minimal if $y \mapsto \log (p_1(y) / p_0(y))$ is nonzero for any point $y$ such that $p_0(y) > 0$. This occurs precisely when $\KL(p_1 \mid p_0) > 0$.
</p>
<p>
The first identity follows from the fact that $\lambda^* = \lambda^*(t)$ satisfies $t = \E_{\lambda^*}[\log\frac{p_1(y)}{p_0(y)}]$ and that
$$
\begin{aligned}
t \lambda^* - \Phi(\lambda^*) & = \E_{\lambda^*}\left[ \lambda^* \log \frac{p_1(y)}{p_0(y)} - \log e^{-\Phi(\lambda^*)} \right]\\
& = \E_{\lambda^*}\left[ \log \left( \exp \left( \lambda^* \log \frac{p_1(y)}{p_0(y)} - \Phi(\lambda^*) \right) \right)\right] \\
& = \E_{\lambda^*} \left[ \log \left( \frac{p_{\lambda^*}(y)}{p_0(y)}\right)\right].
\end{aligned}
$$
</p>
<p>
To derive the second identity, we must first repeat the Chernoff bound for a lower tail probability:
$$
\begin{aligned}
P_1(\Psi_t = 0) & = P_1(L_n(Y^n) < t) \\
& = P_1( e^{-\lambda L_n(Y^n)} > e^{-\lambda t}) \\
& \leq \E_1[\exp(\lambda t - \lambda L_n(Y^n))].
\end{aligned}
$$
The log-MGF is now given by,
$$
\begin{aligned}
\log \E_1[e^{-\lambda L_n(Y^n)}] & = n \int p_1(y)^{1-\frac{\lambda}{n}} p_0(y)^{\frac{\lambda}{n}}  \, dy = n\Phi\left(\frac{1-\lambda}{n}\right)
\end{aligned}
$$
First, when $n=1$,
$$
\begin{aligned}
\inf_{\lambda \in [0, 1]} \lambda t -  \lambda \Phi\left(1-\lambda\right) &= t - \sup_{\lambda \in [0, 1]} \lambda t - \Phi(\lambda) = t - \Phi^*(t)
\end{aligned}
$$
and it is easy to see that
$$
t - \Phi^*(t) = \E_{\lambda^*} \left[\log \frac{p_1(Y)}{p_0(Y)}\right] - \E_{\lambda^*}\left[ \frac{p_{\lambda^*}(Y)}{p_0(Y)} \right] = -\KL(p_{\lambda^*}\mid p_1).
$$
</p>
<p>
<em>One missing step</em>: show that $\lambda^* \in [0, 1]$ whenever $t \in [-\KL(p_0 \mid p_1) , \KL(p_1 \mid p_0)]$ which justifies the restriction in the proof.
</p>
<p>
<em>Remarks</em>. most of the algebra in this proof does not generalize to generic exponential families and instead has to do with specific properties of the interpolating exponential family. This is surprising and interesting! For example, it is a neat symmetry that the upper tail bound for $P_0$ depends on $\Phi(\lambda)$ while the lower tail bound for $P_1$ depends on $\Phi(1-\lambda)$.
</p>
            </div>
        </div>
        <div class="row">
        <p style="text-align: right; width:100%"><i><a href="../miscellany.html">Back...</a></i></p>
        </div>
    </body>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
</html>
