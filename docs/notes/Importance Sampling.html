
<!DOCTYPE html>
    <head>
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="../styles/styles.css">
        <link rel="icon" href="assets/icon.ico" />
        <meta name="google-site-verification" content="gGsyj98Hmr9mZj0-DQzJIAdOP3eJXZJZnM4_NF8Mai8" />
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"> 
        <title>Mara Daniels - MIT</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                ],
                macros: {
                    '\\saeq': '\\{#1\\}',
                    '\\linf': '\\lim_{n \\to \\infty}',
                    '\\Alpha': '\\mathcal{A}',
                    '\\norm': '\\left\\lVert#1\\right\\rVert',
                    '\\ct': 'C^\\infty_c ( #1 )',
                    '\\lloc': 'L^1_{\\mathsf{loc}}( #1 )',
                    '\\Scr': '\\mathscr{S}',
                    '\\tmpr': '\\mathscr{S}',
                    '\\limnf': '\\liminf \\limits_{#1}',
                    '\\limsp': '\\limsup \\limits_{#1}',
                    '\\R': '{\\mathbb R}',
                    '\\C': '{\\mathbb C}',
                    '\\N': '{\\mathbb N}',
                    '\\Q': '{\\mathbb Q}',
                    '\\H': '{\\mathbb H}',
                    '\\S': '{\\mathbb S}',
                    '\\Z': '{\\mathbb Z}',
                    '\\E': '{\\mathbb E}',
                    '\\Var': '\\mathsf{Var}',
                    '\\Cov': '\\mathsf{Cov}',
                    '\\KL': '\\mathsf{KL}',
                    '\\Ent': '\\mathsf{Ent}',
                    '\\F': '{\\mathbb F}',
                    '\\Acl': '\\mathcal{A}',
                    '\\Bcl': '\\mathcal{B}',
                    '\\Ccl': '\\mathcal{C}',
                    '\\Dcl': '\\mathcal{D}',
                    '\\Ecl': '\\mathcal{E}',
                    '\\Fcl': '\\mathcal{F}',
                    '\\Gcl': '\\mathcal{G}',
                    '\\Hcl': '\\mathcal{H}',
                    '\\Jcl': '\\mathcal{J}',
                    '\\Kcl': '\\mathcal{K}',
                    '\\Lcl': '\\mathcal{L}',
                    '\\Mcl': '\\mathcal{M}',
                    '\\Ncl': '\\mathcal{N}',
                    '\\Ocl': '\\mathcal{O}',
                    '\\Pcl': '\\mathcal{P}',
                    '\\Qcl': '\\mathcal{Q}',
                    '\\Rcl': '\\mathcal{R}',
                    '\\Scl': '\\mathcal{S}',
                    '\\Tcl': '\\mathcal{T}',
                    '\\Ucl': '\\mathcal{U}',
                    '\\Vcl': '\\mathcal{V}',
                    '\\Wcl': '\\mathcal{W}',
                    '\\Xcl': '\\mathcal{X}',
                    '\\Ycl': '\\mathcal{Y}',
                    '\\Zcl': '\\mathcal{Z}',
                    '\\diag': '\\mathrm{diag}',
                    '\\supp': '\\mathrm{supp}',
                    '\\diam': '\\mathrm{diam}',
                    '\\sgn': '\\mathrm{sgn}',
                    '\\im': '\\mathrm{Im}',
                    '\\lcm': '\\mathrm{lcm}',
                    '\\aut': '\\mathrm{Aut}',
                    '\\inn': '\\mathrm{Inn}',
                    '\\rg': '\\mathrm{rg}',
                    '\\vol': '\\mathrm{vol}',
                    '\\Pr': '\\mathrm{Pr}',
                    '\\Tr': '\\mathrm{Tr}',
                    '\\eps': '\\varepsilon',
                    '\\charfct': '\\mathds{1}',
                    '\\nullfct': '{\\bf 0}',
                    '\\as': '\\text{a.s.}\\xspace',
                    '\\argmax': '\\mathop{\\mathrm{arg\\,max}}',
                    '\\argmin': '\\mathop{\\mathrm{arg\\,min}}',
                    '\\esssup': '\\mathop{\\mathrm{ess\\,sup}}',
                    '\\th': '\\hat{\\theta}',
                    '\\pto': '\\stackrel{p}{\\longrightarrow}',
                    '\\dto': '\\stackrel{d}{\\longrightarrow}',
                    '\\asto': '\\stackrel{a.s.}{\\longrightarrow}',
                    '\\coloneqq': ':='
                }
            });">
            </script>
    </head>
    <body>
        <div class="row">
            <div class="col-lg-12 col-xl-8 offset-xl-2"> 
            <h1>Importance Sampling</h1>
<hr />
<p>
Importance sampling is a technique to reduce the variance of a <a class="broken" href="#">Monte-Carlo method</a> estimator.
</p>
<h3 id="what-goes-wrong-with-vanilla-monte-carlo">What goes wrong with vanilla Monte-Carlo?</h3>
<p>
In general, Monte-Carlo methods estimate an average using 'naive sampling' across the entire support of the probability distribution. If the quantity of interest only depends on a low probability region, then many samples are required to access that region.
</p>
<p>
For example, let $X_i \sim \pi$ and consider an event $A \subset \supp \pi$. Suppose we wish to estimate the probability of $A$. The random variable $R_i \coloneqq {1}\{X_i \in A \}$ is Bernoulli with probability $p = \pi(A)$, which is also the average $p = \E_\pi[R_i]$. We can estimate the mean with a naive Monte Carlo estimator:
$$
\begin{aligned}
\hat{p}_n = \frac{1}{n}\sum_{i=1}^n 1\{X_i \in A\}
\end{aligned}
$$
which has variance $\Var(\hat{p}_n) = \frac{p(1-p)}{n} \leq \frac{p}{n}$. Equivalently, if we define relative error
$$
\begin{aligned}
E_n \coloneqq \frac{\sqrt{\Var(\hat{p}_n)}}{p} \leq \sqrt{\frac{p}{n}}
\end{aligned}
$$
then we must have $n \geq \frac{1}{p \eps^2}$ samples to get $E_n \leq \eps$. When $p \ll 1$, the procedure is sample intensive!
</p>
<h3 id="importance-sampling">Importance Sampling</h3>
<p>
The method is based on the following key observation. Let $f(x)$ be a generic function and let $\pi(x)$, $g(x)$ be probability densities. The average can be written,
$$
\begin{aligned}
\E_\pi[f(x)] = \E_g\left[f(x) \frac{\pi(x)}{g(x)}\right] = \int f(x) \left(\frac{\pi(x)}{g(x)}\right) g(x)\, dx.
\end{aligned}
$$
The key idea of the method is to apply Monte-Carlo to samples drawn from $g(x)$ instead of $\pi(x)$.
</p>
<p>
A sensible requirement for Monte-Carlo is that the estimator $\tilde{f}(x) = \frac{f(x) \pi(x)}{g(x)}$ has finite second moment: $\E_g[\tilde{f}(x)^2] < \infty$. Similar to <a href="Rejection Sampling.html">Rejection Sampling</a>, a sufficient condition is:
</p>
<ol>
<li>$\sup_x \frac{\pi(x)}{g(x)} \leq C < \infty$
</li>
<li>$\E_\pi[f(x)^2] < \infty$ (this is the same variance condition, stated for the naive Monte-Carlo problem).
</li>
</ol>
<p>
Together, these conditions imply:
$$
\begin{aligned}
\E_g [ \tilde{f}(x)^2 ] =\int \tilde{f}(x)^2 g(x) \, dx = \int f(x)^2 \frac{\pi(x)}{g(x)} \pi(x) \leq C \E_\pi[f(x)^2]
\end{aligned}
$$
</p>
<p>
The <strong>Importance Sampling Estimator</strong> is:
$$
\begin{aligned}
\hat{\mu}_\text{IS} & = \sum_{i=1}^n f(X_i) \frac{\pi(X_i)}{g(X_i)} \qquad X_i \sim g \quad \text{ i.i.d.}
\end{aligned}
$$
</p>
<p>
<em>Example</em>: <strong>Optimal Variance Reduction</strong>. hypothetically, importance sampling could be used to reduce the variance to zero: $$\begin{aligned} g(x) = \frac{f(x) \pi(x)}{\int f(x) \pi(x)\, dx} \quad \implies \quad f(x) \frac{\pi(x)}{g(x)} = \E_\pi[f(x)] \end{aligned}$$
A zero variance estimator is too good to be true! The only way to compute $g(x)$ in this case is to <em>know</em> the normalizing constant $\int f(x) \pi(x) \, dx$, which is exactly what we wanted to estimate.
</p>
<p>
<em>Heuristics</em>: some ways to improve the performance of an IS estimator are:
</p>
<ol>
<li>Use an unnormalized $\pi$, which may be easier to evaluate numerically.
</li>
<li>Replace $\pi$ with $\pi(x) |h(x)|$.
<ul>
<li>In <a href="Self Normalized Importance Sampling.html">Self Normalized Importance Sampling</a>, this technique essentially recovers vanilla Monte-Carlo estimation for the quantity $\E_\pi[h(x)]$.
</li>
</ul>
</li>
<li>Tempering: replace $\pi(x)$ with $\pi(x)^{1/\gamma}$ for large $\gamma \geq 0$. As $\gamma$ increases it smooths out the values of $\pi$. This can be thought of as a 'temperature parameter' for Gibbs sampling.
</li>
</ol>
            </div>
        </div>
        <div class="row">
        <p style="text-align: right; width:100%"><i><a href="../miscellany.html">Back...</a></i></p>
        </div>
    </body>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous"></script>
</html>
